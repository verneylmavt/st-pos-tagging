{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tree import Tree\n",
    "from nltk.tree.prettyprinter import TreePrettyPrinter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Load Vocabulary\n",
    "# ----------------------\n",
    "def load_vocab(model_name):\n",
    "    with open(os.path.join(\"models\", model_name, \"word2idx.json\"), 'r') as json_file:\n",
    "        word2idx = json.load(json_file)\n",
    "    with open(os.path.join(\"models\", model_name, \"idx2pos.json\"), 'r') as json_file:\n",
    "        idx2pos = json.load(json_file)\n",
    "        idx2pos = {int(k): v for k, v in idx2pos.items()}\n",
    "    return word2idx, idx2pos\n",
    "\n",
    "# ----------------------\n",
    "# Load Model\n",
    "# ----------------------\n",
    "def load_model(model_name, vocab):\n",
    "    class SPositionalEncoding(nn.Module):\n",
    "        def __init__(self, embed_size, max_len=5000):\n",
    "            super(SPositionalEncoding, self).__init__()\n",
    "            pe = torch.zeros(max_len, embed_size)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            if embed_size % 2 == 1:\n",
    "                pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "            else:\n",
    "                pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0)\n",
    "            self.register_buffer('pe', pe)\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:, :x.size(1), :]\n",
    "            return x\n",
    "    class STransformerE(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, output_dim, padding_idx, embedding_matrix, dropout=0.1):\n",
    "            super(STransformerE, self).__init__()\n",
    "            # Embedding Layer\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "            self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "            # Positional Encoding Layer\n",
    "            self.pos_encoder = SPositionalEncoding(embed_size)\n",
    "            # Transformer Encoder Layer\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=hidden_dim, \n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(embed_size))\n",
    "            # Dropout Layer\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            # Fully Connected Layer\n",
    "            self.fc = nn.Linear(embed_size, output_dim)\n",
    "        def forward(self, x):\n",
    "            embedded = self.embedding(x)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            embedded = self.dropout(embedded)\n",
    "            src_key_padding_mask = (x == self.embedding.padding_idx)\n",
    "            transformer_output = self.transformer_encoder(\n",
    "                embedded, \n",
    "                src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "            transformer_output = self.dropout(transformer_output)\n",
    "            logits = self.fc(transformer_output)\n",
    "            return logits\n",
    "    embedding_matrix = torch.load(os.path.join(\"models\", model_name, \"embedding-matrix.pth\"), weights_only=True, map_location=torch.device('cpu'))\n",
    "    model = STransformerE(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_size=200,\n",
    "        num_heads=8,\n",
    "        hidden_dim=512,\n",
    "        num_layers=2,\n",
    "        output_dim=47,\n",
    "        padding_idx=0,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        dropout=0.16426146772147993\n",
    "    )\n",
    "    model.load_state_dict(torch.load(os.path.join(\"models\", model_name, \"model-state.pt\"), weights_only=True, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "\n",
    "# ----------------------\n",
    "# Predict Function\n",
    "# ----------------------\n",
    "def predict_pos_tag(model, word2idx, idx2pos, sequence):\n",
    "    model.eval()\n",
    "    pos_descriptions = {\n",
    "        \"CC\": \"Coordinating Conjunction\",\n",
    "        \"CD\": \"Cardinal Number\",\n",
    "        \"DT\": \"Determiner\",\n",
    "        \"EX\": \"Existential 'There'\",\n",
    "        \"FW\": \"Foreign Word\",\n",
    "        \"IN\": \"Preposition or Subordinating Conjunction\",\n",
    "        \"JJ\": \"Adjective\",\n",
    "        \"JJR\": \"Adjective (Comparative)\",\n",
    "        \"JJS\": \"Adjective (Superlative)\",\n",
    "        \"LS\": \"List Item Marker\",\n",
    "        \"MD\": \"Modal\",\n",
    "        \"NN\": \"Noun (Singular or Mass)\",\n",
    "        \"NNS\": \"Noun (Plural)\",\n",
    "        \"NNP\": \"Proper Noun (Singular)\",\n",
    "        \"NNPS\": \"Proper Noun (Plural)\",\n",
    "        \"PDT\": \"Predeterminer\",\n",
    "        \"POS\": \"Possessive Ending\",\n",
    "        \"PRP\": \"Personal Pronoun\",\n",
    "        \"PRP$\": \"Possessive Pronoun\",\n",
    "        \"RB\": \"Adverb\",\n",
    "        \"RBR\": \"Adverb (Comparative)\",\n",
    "        \"RBS\": \"Adverb (Superlative)\",\n",
    "        \"RP\": \"Particle\",\n",
    "        \"SYM\": \"Symbol\",\n",
    "        \"TO\": \"to\",\n",
    "        \"UH\": \"Interjection\",\n",
    "        \"VB\": \"Verb (Base Form)\",\n",
    "        \"VBD\": \"Verb (Past Tense)\",\n",
    "        \"VBG\": \"Verb (Gerund or Present Participle)\",\n",
    "        \"VBN\": \"Verb (Past Participle)\",\n",
    "        \"VBP\": \"Verb (Non-3rd-Person Singular Present)\",\n",
    "        \"VBZ\": \"Verb (3rd Person Singular Present)\",\n",
    "        \"WDT\": \"Wh-Determiner\",\n",
    "        \"WP\": \"Wh-Pronoun\",\n",
    "        \"WP$\": \"Possessive Wh-Pronoun\",\n",
    "        \"WRB\": \"Wh-Adverb\"\n",
    "    }\n",
    "    if isinstance(sequence, str):\n",
    "        words = sequence.split()\n",
    "    elif isinstance(sequence, list):\n",
    "        words = sequence\n",
    "    else:\n",
    "        raise ValueError(\"Input sequence must be a string or list of words\")\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    word_indices = [word2idx.get(word, word2idx['<UNK>']) for word in words_lower]\n",
    "    input_tensor = torch.tensor([word_indices], dtype=torch.long)\n",
    "    lengths = torch.tensor([len(word_indices)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    predicted_pos_indices = predictions[0][:lengths[0]].cpu().numpy()\n",
    "    predicted_pos_tags = [idx2pos[idx] for idx in predicted_pos_indices]\n",
    "    word_pos_pairs = list(zip(words, predicted_pos_tags))\n",
    "    tree = Tree('S', [Tree(pos, [word]) for word, pos in word_pos_pairs])\n",
    "    ordered_unique_pos = []\n",
    "    for pos in predicted_pos_tags:\n",
    "        if pos not in ordered_unique_pos:\n",
    "            ordered_unique_pos.append(pos)    \n",
    "    description = {pos: pos_descriptions.get(pos, \"Unknown POS tag\") for pos in ordered_unique_pos}\n",
    "    return tree, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# User Interface\n",
    "# ----------------------\n",
    "def create_pos_tagging_interface(model_name=\"s_transformer-e\"):\n",
    "    word2idx, idx2pos = load_vocab(model_name)\n",
    "    model = load_model(model_name, word2idx)\n",
    "\n",
    "    title = widgets.Label(value=\"POS Tagging\")\n",
    "    text_input = widgets.Textarea(description=\"Sentence:\", placeholder=\"e.g. The quick brown fox jumps over the lazy dog.\")\n",
    "    output_area = widgets.Textarea(value=\"Result:\", layout=widgets.Layout(height='200px', width='500px'), disabled=True)\n",
    "    tag_button = widgets.Button(description=\"Tag\")\n",
    "    \n",
    "    def on_infer_clicked(b):\n",
    "        input_text = text_input.value\n",
    "        if input_text.strip():\n",
    "            tree, description = predict_pos_tag(model, word2idx, idx2pos, input_text)\n",
    "            result = TreePrettyPrinter(tree).text()\n",
    "            description_result = \"\\n\".join([f\"{k}: {v}\" for k, v in description.items()])\n",
    "            output_area.value = f\"Result:\\n{result}\\n{description_result}\"\n",
    "        else:\n",
    "            output_area.value = \"Please enter some text.\"\n",
    "    \n",
    "    tag_button.on_click(on_infer_clicked)\n",
    "    \n",
    "    display(widgets.VBox([title, text_input, tag_button, output_area]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb6bdbabc0a42a1bd63b4253bf7588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='POS Tagging'), Textarea(value='', description='Sentence:', placeholder='e.g. The q…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_pos_tagging_interface(\"s_transformer-e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
